/*
 * Copyright 2016 LinkedIn Corp.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"); you may not
 * use this file except in compliance with the License. You may obtain a copy of
 * the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 * License for the specific language governing permissions and limitations under
 * the License.
 */

package com.linkedin.drelephant;

import com.linkedin.drelephant.Executors.IExecutorService;
import com.linkedin.drelephant.Executors.QuartzExecutorService;
import com.linkedin.drelephant.Executors.ThreadPoolExecutorService;
import com.linkedin.drelephant.analysis.AnalyticJobGenerator;
import com.linkedin.drelephant.analysis.AnalyticJobGeneratorHadoop2;
import com.linkedin.drelephant.analysis.HDFSContext;
import com.linkedin.drelephant.analysis.HadoopSystemContext;
import com.linkedin.drelephant.security.HadoopSecurity;
import com.linkedin.drelephant.util.Utils;
import controllers.MetricsController;
import org.apache.commons.lang.exception.ExceptionUtils;
import org.apache.hadoop.conf.Configuration;
import org.apache.log4j.Logger;

import java.security.PrivilegedAction;
import java.util.concurrent.atomic.AtomicBoolean;


/**
 * The class that runs the Dr. Elephant daemon
 */
public class ElephantRunner implements Runnable {
    private static final Logger logger = Logger.getLogger(ElephantRunner.class);

    private static final long FETCH_INTERVAL = 60 * 1000;     // Interval between fetches
    private static final long RETRY_INTERVAL = 60 * 1000;     // Interval between retries
    private static final int EXECUTOR_NUM = 5;                // The number of executor threads to analyse the jobs

    private static final String FETCH_INTERVAL_KEY = "drelephant.analysis.fetch.interval";
    private static final String RETRY_INTERVAL_KEY = "drelephant.analysis.retry.interval";
    private static final String EXECUTOR_NUM_KEY = "drelephant.analysis.thread.count";
    private static final String EXECUTOR_SERVICE = "drelephant.executor.service";

    private AtomicBoolean _running = new AtomicBoolean(true);
    private long _fetchInterval;
    private long _retryInterval;
    private int _executorNum;
    private HadoopSecurity _hadoopSecurity;
    private AnalyticJobGenerator _analyticJobGenerator;
    private IExecutorService executorService;


    private static ElephantRunner elephantRunner;

    private ElephantRunner() {
    }

    public static ElephantRunner getInstance() {
        if (elephantRunner == null) {
            elephantRunner = new ElephantRunner();
        }
        return elephantRunner;
    }

    public AtomicBoolean getRunningStatus() {
        return _running;
    }

    public int getExecutorNum() {
        return _executorNum;
    }

    public long getFetchInterval() {
        return _fetchInterval;
    }

    public long getRetryInterval() {
        return _retryInterval;
    }

    public HadoopSecurity getHadoopSecurity() {
        return _hadoopSecurity;
    }

    public AnalyticJobGenerator getAnalyticJobGenerator() {
        return _analyticJobGenerator;
    }

    public IExecutorService getDistributedExecutorService() {
        return executorService;
    }

    private void loadGeneralConfiguration() {
        Configuration configuration = ElephantContext.instance().getGeneralConf();

        _executorNum = Utils.getNonNegativeInt(configuration, EXECUTOR_NUM_KEY, EXECUTOR_NUM);
        _fetchInterval = Utils.getNonNegativeLong(configuration, FETCH_INTERVAL_KEY, FETCH_INTERVAL);
        _retryInterval = Utils.getNonNegativeLong(configuration, RETRY_INTERVAL_KEY, RETRY_INTERVAL);
    }

    private void loadAnalyticJobGenerator() {
        if (HadoopSystemContext.isHadoop2Env()) {
            _analyticJobGenerator = new AnalyticJobGeneratorHadoop2();
        } else {
            throw new RuntimeException("Unsupported Hadoop major version detected. It is not 2.x.");
        }

        try {
            _analyticJobGenerator.configure(ElephantContext.instance().getGeneralConf());
        } catch (Exception e) {
            logger.error("Error occurred when configuring the analysis provider.", e);
            throw new RuntimeException(e);
        }
    }

    private void loadExecutorService() {

        Configuration configuration = ElephantContext.instance().getGeneralConf();
        String service = configuration.get(EXECUTOR_SERVICE);
        if (service.equalsIgnoreCase("threadPoolExecutorService"))
            executorService = new ThreadPoolExecutorService();
        else if (service.equalsIgnoreCase("quartzExecutorService"))
            executorService = new QuartzExecutorService();
    }

    @Override
    public void run() {
        logger.info("Dr.elephant has started");
        try {
            _hadoopSecurity = HadoopSecurity.getInstance();
            _hadoopSecurity.doAs(new PrivilegedAction<Void>() {
                @Override
                public Void run() {
                    HDFSContext.load();
                    loadGeneralConfiguration();
                    loadAnalyticJobGenerator();
                    loadExecutorService();
                    ElephantContext.init();

                    // Initialize the metrics registries.
                    MetricsController.init();

                    logger.info("executor num is " + _executorNum);
                    if (_executorNum < 1) {
                        throw new RuntimeException("Must have at least 1 worker thread.");
                    }

                    executorService.startService();

                    logger.info("Main thread is terminated.");
                    return null;
                }
            });
        } catch (Exception e) {
            logger.error(e.getMessage());
            logger.error(ExceptionUtils.getStackTrace(e));
        }
    }

    public void kill() {
        _running.set(false);
        executorService.stopService();
    }
}





